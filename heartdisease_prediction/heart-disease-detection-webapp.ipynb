{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8003944,"sourceType":"datasetVersion","datasetId":4713683}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"5cbd36fb-9c5d-4083-8ac2-51c810ef7929","_cell_guid":"31594221-8c0c-4326-b990-084eee5e0a3f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.569152Z","iopub.execute_input":"2024-04-02T05:45:26.57011Z","iopub.status.idle":"2024-04-02T05:45:26.581333Z","shell.execute_reply.started":"2024-04-02T05:45:26.570058Z","shell.execute_reply":"2024-04-02T05:45:26.580134Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/heart-disease-data-compiled-from-uci/UCI_Heart_Disease_Dataset_Combined.csv\")","metadata":{"_uuid":"718b6dee-478c-4a7f-84eb-605e6d44efb7","_cell_guid":"23eed174-be1f-4d15-b304-1634a8a45f0a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.584027Z","iopub.execute_input":"2024-04-02T05:45:26.584526Z","iopub.status.idle":"2024-04-02T05:45:26.601751Z","shell.execute_reply.started":"2024-04-02T05:45:26.584475Z","shell.execute_reply":"2024-04-02T05:45:26.600131Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"b52b7c46-082f-4fc7-a1f2-d59e592ccffe","_cell_guid":"bc71cba4-df41-432e-9275-6aa3ca39d99b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.603715Z","iopub.execute_input":"2024-04-02T05:45:26.604146Z","iopub.status.idle":"2024-04-02T05:45:26.623074Z","shell.execute_reply.started":"2024-04-02T05:45:26.604112Z","shell.execute_reply":"2024-04-02T05:45:26.621673Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"_uuid":"db40b28b-a8f0-4aa4-ba0c-d23df88c3499","_cell_guid":"45f407c9-191c-4696-8dec-a5b6c85472d8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.625225Z","iopub.execute_input":"2024-04-02T05:45:26.625748Z","iopub.status.idle":"2024-04-02T05:45:26.63405Z","shell.execute_reply.started":"2024-04-02T05:45:26.6257Z","shell.execute_reply":"2024-04-02T05:45:26.632698Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"_uuid":"f9a57ded-3cdd-495c-adcf-6333145b365c","_cell_guid":"40e308b5-7816-49a8-92a6-490ce2244dc9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.638355Z","iopub.execute_input":"2024-04-02T05:45:26.63883Z","iopub.status.idle":"2024-04-02T05:45:26.653866Z","shell.execute_reply.started":"2024-04-02T05:45:26.638795Z","shell.execute_reply":"2024-04-02T05:45:26.652668Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"_uuid":"a0accb25-a3e8-4146-a383-5d7a915ad6ee","_cell_guid":"80db2123-fc72-40c0-94fc-8b2bd85fe068","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.655706Z","iopub.execute_input":"2024-04-02T05:45:26.656081Z","iopub.status.idle":"2024-04-02T05:45:26.668233Z","shell.execute_reply.started":"2024-04-02T05:45:26.656048Z","shell.execute_reply":"2024-04-02T05:45:26.666701Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"861e7d43-3bb5-48d9-9bba-ef653522d3b3","_cell_guid":"ec5ca958-0a52-41a5-b736-7e8c956fc1d8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.670529Z","iopub.execute_input":"2024-04-02T05:45:26.670924Z","iopub.status.idle":"2024-04-02T05:45:26.689518Z","shell.execute_reply.started":"2024-04-02T05:45:26.670891Z","shell.execute_reply":"2024-04-02T05:45:26.687985Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"duplicate_rows_df = df[df.duplicated()]\nprint(\"\\nDuplicate Rows except the first occurrence based on all columns are:\")\nprint(duplicate_rows_df)","metadata":{"_uuid":"b94b0857-9c6b-43a0-9df6-3ccc5114b242","_cell_guid":"b30be096-56f9-498a-8a46-d4874fe5170e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.691239Z","iopub.execute_input":"2024-04-02T05:45:26.69165Z","iopub.status.idle":"2024-04-02T05:45:26.712981Z","shell.execute_reply.started":"2024-04-02T05:45:26.691617Z","shell.execute_reply":"2024-04-02T05:45:26.711727Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop_duplicates()\nprint(\"\\nDataFrame after removing duplicates:\")\nprint(df)","metadata":{"_uuid":"c693a87a-e70f-49cc-970c-504101b06605","_cell_guid":"80bc5ff1-83ce-4a51-99a6-3490d1f8267a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.715127Z","iopub.execute_input":"2024-04-02T05:45:26.716047Z","iopub.status.idle":"2024-04-02T05:45:26.731178Z","shell.execute_reply.started":"2024-04-02T05:45:26.716Z","shell.execute_reply":"2024-04-02T05:45:26.729898Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nplt.hist(df[\"HeartDisease\"])","metadata":{"_uuid":"079642ae-816d-4869-8198-0f329ff1e7ee","_cell_guid":"032c5b01-9e7a-4570-8842-e57f0ed9d0c6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:26.73262Z","iopub.execute_input":"2024-04-02T05:45:26.733171Z","iopub.status.idle":"2024-04-02T05:45:27.059253Z","shell.execute_reply.started":"2024-04-02T05:45:26.733139Z","shell.execute_reply":"2024-04-02T05:45:27.058392Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=['ChestPainType'])\ndf.head()","metadata":{"_uuid":"98553880-1fce-4ef7-b861-1e3e391497df","_cell_guid":"e5ec3c22-500f-44d3-a60e-1cfe87650356","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:27.062882Z","iopub.execute_input":"2024-04-02T05:45:27.063394Z","iopub.status.idle":"2024-04-02T05:45:27.086358Z","shell.execute_reply.started":"2024-04-02T05:45:27.063363Z","shell.execute_reply":"2024-04-02T05:45:27.085067Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nX = df.drop('HeartDisease', axis=1)\ny = df['HeartDisease']\n\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=42)\n\nclassifiers = {\n    'Logistic Regression': LogisticRegression(),\n    'Random Forest': RandomForestClassifier(),\n#     'SVM': SVC(),\n    'XGBoost': XGBClassifier(),\n    'LightGBM': LGBMClassifier(),\n    'CatBoost': CatBoostClassifier(silent=True),\n    'GradientBoost':GradientBoostingClassifier(),\n    'ExtraTrees':ExtraTreesClassifier()\n}\n\n# Train and test each classifier\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    test_predictions = clf.predict(X_test)\n    test_accuracy = accuracy_score(y_test, test_predictions)\n    test_precision = precision_score(y_test, test_predictions)\n    test_recall = recall_score(y_test, test_predictions)\n    test_f1 = f1_score(y_test, test_predictions)\n    print(f'{name} test Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}')\n    \n    # val_predictions = clf.predict(X_val)\n    # val_accuracy = accuracy_score(y_val, val_predictions)\n    # val_precision = precision_score(y_val, val_predictions)\n    # val_recall = recall_score(y_val, val_predictions)\n    # val_f1 = f1_score(y_val, val_predictions)\n    # print(f'{name} validation Accuracy: {val_accuracy}, Precision: {val_precision}, Recall: {val_recall}, F1 Score: {val_f1}')","metadata":{"_uuid":"0a117f6e-e7be-42d0-8539-15409218da03","_cell_guid":"a80e3043-a039-4aed-83e9-4240be432719","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:27.088246Z","iopub.execute_input":"2024-04-02T05:45:27.088703Z","iopub.status.idle":"2024-04-02T05:45:30.725808Z","shell.execute_reply.started":"2024-04-02T05:45:27.088663Z","shell.execute_reply":"2024-04-02T05:45:30.724636Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\n\nX_train = X_train.astype('float32')\ny_train = y_train.astype('float32')\nX_test = X_test.astype('float32')\ny_test = y_test.astype('float32')\n# Define the model architecture\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # adjust this according to your problem\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # adjust this according to your problem\n\n# Define the checkpoint callback\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_model.keras\", save_best_only=True)\n\n# Train the model\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=[checkpoint_cb])\n\n# Evaluate the model on the train set\ntrain_loss, train_acc = model.evaluate(X_train, y_train, verbose=2)\nprint('\\nTrain accuracy:', train_acc)\n\n# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\nprint('\\nTest accuracy:', test_acc)","metadata":{"_uuid":"10905ba8-5af0-40ae-8751-af40ec90031d","_cell_guid":"68d98c00-2922-4880-8c26-6a87b7a79132","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:30.727657Z","iopub.execute_input":"2024-04-02T05:45:30.728354Z","iopub.status.idle":"2024-04-02T05:45:51.298635Z","shell.execute_reply.started":"2024-04-02T05:45:30.728311Z","shell.execute_reply":"2024-04-02T05:45:51.297415Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nX_train=X_train.astype('int')\nX_test=X_test.astype('int')\nX_val=X_val.astype('int')\ndef objective(trial):\n    param = {\n        'iterations' : trial.suggest_int('iterations', 50, 300),\n        'depth' : trial.suggest_int('depth', 4, 10),\n        'learning_rate' : trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'random_strength' : trial.suggest_int('random_strength', 0, 100),\n        'bagging_temperature' : trial.suggest_float('bagging_temperature', 0.01, 100.00, log=True),\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'od_wait' : trial.suggest_int('od_wait', 10, 50)\n    }\n    model = CatBoostClassifier(**param)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n    preds = model.predict(X_test)\n    accuracy = f1_score(y_test, preds,average=\"weighted\")\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1000)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = CatBoostClassifier(**best_params)\nbest_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))\n\n# Now let's use the model with the best parameters on the validation set\nval_preds = best_model.predict(X_val)\n\n# Check the accuracy and F1 score of the best model on the validation set\nprint(\"Validation Accuracy: %f\" % accuracy_score(y_val, val_preds))\nprint(\"Validation F1 Score: %f\" % f1_score(y_val, val_preds, average='weighted'))","metadata":{"_uuid":"c5bb0f7f-dab6-4691-87c2-bccab13966ae","_cell_guid":"08c988d7-99be-494a-882b-3a435a1fc9ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T05:45:51.302348Z","iopub.execute_input":"2024-04-02T05:45:51.302701Z","iopub.status.idle":"2024-04-02T06:01:16.901077Z","shell.execute_reply.started":"2024-04-02T05:45:51.302672Z","shell.execute_reply":"2024-04-02T06:01:16.900059Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model.save_model(\"CB\",format=\"cbm\")","metadata":{"_uuid":"716c1d12-03e3-4e64-8b77-8a4c732947d8","_cell_guid":"790f693c-ad15-4239-bb1a-d87fd20979b6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:01:16.902577Z","iopub.execute_input":"2024-04-02T06:01:16.903115Z","iopub.status.idle":"2024-04-02T06:01:16.922229Z","shell.execute_reply.started":"2024-04-02T06:01:16.903085Z","shell.execute_reply":"2024-04-02T06:01:16.921152Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef objective(trial):\n    param = {\n        'C': trial.suggest_float('C', 1e-5, 100,log=True),\n        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n        # 'penalty': trial.suggest_categorical('penalty', ['l2','none']),\n        'solver': trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n        'max_iter': trial.suggest_int('max_iter', 50, 200),\n        'tol': trial.suggest_float('tol', 1e-5, 1e-1),\n        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n        'warm_start': trial.suggest_categorical('warm_start', [True, False]),\n        'l1_ratio': trial.suggest_float('l1_ratio', 0, 1)\n    }\n    model = LogisticRegression(**param)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    accuracy = f1_score(y_test, preds, average='weighted')\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1000)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = LogisticRegression(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))\n\n# Now let's use the model with the best parameters on the validation set\nval_preds = best_model.predict(X_val)\n\n# Check the accuracy and F1 score of the best model on the validation set\nprint(\"Validation Accuracy: %f\" % accuracy_score(y_val, val_preds))\nprint(\"Validation F1 Score: %f\" % f1_score(y_val, val_preds, average='weighted'))","metadata":{"_uuid":"77ada293-8c83-457a-a38f-6143071946ef","_cell_guid":"499126bb-7927-4c67-9b9e-9ee5b954a3d6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:01:16.923803Z","iopub.execute_input":"2024-04-02T06:01:16.924437Z","iopub.status.idle":"2024-04-02T06:04:49.220334Z","shell.execute_reply.started":"2024-04-02T06:01:16.924376Z","shell.execute_reply":"2024-04-02T06:04:49.219138Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\npickle.dump(best_model, open(\"LR\", 'wb'))","metadata":{"_uuid":"1dc512c4-68a0-4d40-8f9a-d6cd4d2ec75e","_cell_guid":"f8bbdc5a-f773-4dc1-b31e-817bbe1f4458","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:04:49.222321Z","iopub.execute_input":"2024-04-02T06:04:49.223133Z","iopub.status.idle":"2024-04-02T06:04:49.228933Z","shell.execute_reply.started":"2024-04-02T06:04:49.223091Z","shell.execute_reply":"2024-04-02T06:04:49.227665Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1,log=True),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 5),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n    }\n    model = GradientBoostingClassifier(**param)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    accuracy = f1_score(y_test, preds,average=\"weighted\")\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1000)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = GradientBoostingClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))\n\n# Now let's use the model with the best parameters on the validation set\nval_preds = best_model.predict(X_val)\n\n# Check the accuracy and F1 score of the best model on the validation set\nprint(\"Validation Accuracy: %f\" % accuracy_score(y_val, val_preds))\nprint(\"Validation F1 Score: %f\" % f1_score(y_val, val_preds, average='weighted'))","metadata":{"_uuid":"be078a4a-c67d-4558-a646-429cfb8aeea9","_cell_guid":"e1af3ce1-29f6-4fae-aa00-c55fdf25e8a2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:04:49.231039Z","iopub.execute_input":"2024-04-02T06:04:49.231862Z","iopub.status.idle":"2024-04-02T06:23:40.676938Z","shell.execute_reply.started":"2024-04-02T06:04:49.23182Z","shell.execute_reply":"2024-04-02T06:23:40.67456Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump(best_model, open(\"GB\", 'wb'))","metadata":{"_uuid":"612ec640-4b01-40af-b896-6d21f694a964","_cell_guid":"aac5fcf0-9170-4919-aa0f-60903d33102f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:23:40.680581Z","iopub.execute_input":"2024-04-02T06:23:40.68138Z","iopub.status.idle":"2024-04-02T06:23:40.709848Z","shell.execute_reply.started":"2024-04-02T06:23:40.681311Z","shell.execute_reply":"2024-04-02T06:23:40.707379Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n        'max_depth': trial.suggest_int('max_depth', 2, 32),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 5),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n        'max_features': trial.suggest_int('max_features', 1,1500),\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n    }\n    model = RandomForestClassifier(**param)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    accuracy = f1_score(y_val, preds,average=\"weighted\")\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = RandomForestClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))","metadata":{"_uuid":"4bbb92c4-e55f-4f63-997f-93c1b837b5b6","_cell_guid":"833f585e-a30d-4cf7-b5af-cfb488df4d65","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:23:40.713034Z","iopub.execute_input":"2024-04-02T06:23:40.713813Z","iopub.status.idle":"2024-04-02T06:32:12.082126Z","shell.execute_reply.started":"2024-04-02T06:23:40.713751Z","shell.execute_reply":"2024-04-02T06:32:12.080501Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump(best_model, open(\"RF\", 'wb'))","metadata":{"_uuid":"8a1e793c-51ec-4133-919f-6c8afc68a8bd","_cell_guid":"c622a4db-fe9b-4f1b-a2d1-7a0a9da65fea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:32:12.083975Z","iopub.execute_input":"2024-04-02T06:32:12.084386Z","iopub.status.idle":"2024-04-02T06:32:12.142928Z","shell.execute_reply.started":"2024-04-02T06:32:12.084351Z","shell.execute_reply":"2024-04-02T06:32:12.141622Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tabnanny import verbose\nimport optuna\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef objective(trial):\n    param = {\n        'num_leaves': trial.suggest_int('num_leaves', 2, 500),\n        'max_depth': trial.suggest_int('max_depth', 2, 128),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0,log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 1,log=True),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.1),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10,log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10,log=True)\n    }\n    model = LGBMClassifier(**param,verbose=-1)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    accuracy = f1_score(y_val, preds, average=\"weighted\")\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1000)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = LGBMClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))","metadata":{"_uuid":"9d222f17-10b3-4460-9159-48f9d5dd01b0","_cell_guid":"cb27441a-7555-49a1-b822-846928b5ae85","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:32:12.144357Z","iopub.execute_input":"2024-04-02T06:32:12.144797Z","iopub.status.idle":"2024-04-02T06:40:33.654124Z","shell.execute_reply.started":"2024-04-02T06:32:12.144754Z","shell.execute_reply":"2024-04-02T06:40:33.653107Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump(best_model, open(\"LGBM\", 'wb'))","metadata":{"_uuid":"c1efc30b-1186-403c-ad5f-b923c43c6b84","_cell_guid":"302afe78-ff2e-422f-b5c5-d1274ad08eb2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:40:33.655631Z","iopub.execute_input":"2024-04-02T06:40:33.656537Z","iopub.status.idle":"2024-04-02T06:40:33.686107Z","shell.execute_reply.started":"2024-04-02T06:40:33.656503Z","shell.execute_reply":"2024-04-02T06:40:33.684688Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n        'max_depth': trial.suggest_int('max_depth', 2, 32),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 5),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n    }\n    model = ExtraTreesClassifier(**param)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    accuracy = f1_score(y_val, preds, average=\"weighted\")\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = ExtraTreesClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))","metadata":{"_uuid":"6d87755e-1654-43cf-aed3-621d0181a44e","_cell_guid":"83743874-7bc1-4b71-89da-d68a60a1915a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T06:40:33.687909Z","iopub.execute_input":"2024-04-02T06:40:33.688299Z","iopub.status.idle":"2024-04-02T07:10:47.208168Z","shell.execute_reply.started":"2024-04-02T06:40:33.688267Z","shell.execute_reply":"2024-04-02T07:10:47.205864Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump(best_model, open(\"ET\", 'wb'))","metadata":{"_uuid":"43f59dd9-fdc5-43cd-8c53-048c3c3f3c9a","_cell_guid":"666e8436-7bb8-4adc-ac0d-99c5d71d715c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:10:47.213426Z","iopub.execute_input":"2024-04-02T07:10:47.213957Z","iopub.status.idle":"2024-04-02T07:10:47.276387Z","shell.execute_reply.started":"2024-04-02T07:10:47.213916Z","shell.execute_reply":"2024-04-02T07:10:47.274878Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n        'max_depth': trial.suggest_int('max_depth', 2, 32),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0,log=True),\n        'gamma': trial.suggest_float('gamma', 0.1, 1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1, step=0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10,log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10,log=True)\n    }\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    accuracy = f1_score(y_val, preds,average=\"weighted\")\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1000)\n\n# Best hyperparameters\nbest_params = study.best_params\n\n# Fit the model with best hyperparameters\nbest_model = XGBClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\n# Make predictions \npreds = best_model.predict(X_test)\n\n# Check the accuracy and F1 score of the model\nprint(\"Model Accuracy: %f\" % accuracy_score(y_test, preds))\nprint(\"Model F1 Score: %f\" % f1_score(y_test, preds, average='weighted'))","metadata":{"_uuid":"4798e9ba-af5b-44ac-a5b6-d07cab87c795","_cell_guid":"b1a868b5-5fd1-4e04-8bd1-ea8baf6df55e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump(best_model, open(\"XGB\", 'wb'))","metadata":{"_uuid":"bb69e94b-72b1-4cb4-9e97-258968ea119a","_cell_guid":"23fd7493-0d4d-4617-a692-b38472f239a0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:43:28.282299Z","iopub.execute_input":"2024-04-02T07:43:28.28283Z","iopub.status.idle":"2024-04-02T07:43:28.31855Z","shell.execute_reply.started":"2024-04-02T07:43:28.282791Z","shell.execute_reply":"2024-04-02T07:43:28.317304Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle","metadata":{"_uuid":"1e654531-1008-4775-af1a-2e1674fb500e","_cell_guid":"3df3e47d-1f80-48cc-b606-60b3fd9854ea","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlr=pickle.load(open('LR', 'rb'))\ngb=pickle.load(open(\"GB\", 'rb'))\nrf=pickle.load(open(\"RF\", 'rb'))\nlgbm=pickle.load(open(\"LGBM\", 'rb'))\net=pickle.load(open(\"ET\", 'rb'))\nxgb=pickle.load(open(\"XGB\", 'rb'))","metadata":{"_uuid":"d69418bb-9bd7-4450-9c37-3cbeb43734e9","_cell_guid":"7db10df9-22f6-4da1-8271-096a13501a7b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:43:47.914458Z","iopub.execute_input":"2024-04-02T07:43:47.915898Z","iopub.status.idle":"2024-04-02T07:43:49.350935Z","shell.execute_reply.started":"2024-04-02T07:43:47.915837Z","shell.execute_reply":"2024-04-02T07:43:49.34977Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cb=CatBoostClassifier()\ncb.load_model(\"CB\")","metadata":{"_uuid":"5c5992f3-2b3b-4247-a0bd-51b7b8e5223b","_cell_guid":"7b4a7fda-3e9b-4508-875c-7b2f28524665","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:43:56.831378Z","iopub.execute_input":"2024-04-02T07:43:56.832561Z","iopub.status.idle":"2024-04-02T07:43:56.856716Z","shell.execute_reply.started":"2024-04-02T07:43:56.832513Z","shell.execute_reply":"2024-04-02T07:43:56.855459Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p1=cb.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=cb.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")\n\np1=lr.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=lr.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")\n\np1=gb.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=gb.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")\n\np1=rf.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=rf.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")\n\np1=lgbm.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=lgbm.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")\n\np1=et.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=et.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")\n\np1=xgb.predict(X_val)\nprint(f1_score(p1,y_val,average=\"weighted\"))\nprint(\"\\n\")\np2=xgb.predict(X_test)\nprint(f1_score(p2,y_test,average=\"weighted\"))\nprint(\"\\n\")","metadata":{"_uuid":"4fb6f402-b736-4848-bf46-113fad59bb0c","_cell_guid":"494a5c0d-99d8-4f85-bdbb-ac03a2c792e6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:44:00.364891Z","iopub.execute_input":"2024-04-02T07:44:00.366152Z","iopub.status.idle":"2024-04-02T07:44:00.709556Z","shell.execute_reply.started":"2024-04-02T07:44:00.366074Z","shell.execute_reply":"2024-04-02T07:44:00.708Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.optimize import minimize\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\npreds_cb = cb.predict(X_val)\npreds_lr = lr.predict(X_val)\npreds_gb = gb.predict(X_val)\npreds_rf = rf.predict(X_val)\npreds_lgbm = lgbm.predict(X_val)\npreds_et = et.predict(X_val)\npreds_xgb = xgb.predict(X_val)\n\n# Stack predictions\npreds = np.vstack([preds_cb, preds_lr, preds_gb, preds_rf, preds_lgbm,preds_et,preds_xgb])\n\ndef loss_func(weights):\n    \n    final_prediction = np.average(preds, axis=0, weights=weights)\n    final_prediction = [1 if prob > 0.5 else 0 for prob in final_prediction]\n    return 1 - f1_score(y_val, final_prediction, average='weighted')\n\n# The algorithm needs a starting value, let's start with equal weights\nstarting_values = [1/7,1/7,1/7,1/7,1/7,1/7,1/7]\n\n# Our weights are bound between 0 and 1\nbounds = [(0, 1)]*7\n\n# We want our weights to sum to 1\ncons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n\n# We use 'SLSQP' as our solver, SLSQP stands for Sequential Least Squares Programming\nres = minimize(loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n\nprint('Ensemble Weights: {weights}'.format(weights=res['x']))\n\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n\n# Get the predictions from each model on the test set\npreds_cb = cb.predict(X_test)\npreds_lr = lr.predict(X_test)\npreds_gb = gb.predict(X_test)\npreds_rf = rf.predict(X_val)\npreds_lgbm = lgbm.predict(X_val)\npreds_et = et.predict(X_val)\npreds_xgb = xgb.predict(X_val)\n\n# Stack the predictions together\npreds = np.vstack([preds_cb, preds_lr, preds_gb, preds_rf, preds_lgbm,preds_et,preds_xgb])\n\n# Calculate the weighted average of predictions\nfinal_preds = np.average(preds, axis=0, weights=res['x'])\n\n# Convert probabilities to class labels\nfinal_preds = [1 if prob > 0.5 else 0 for prob in final_preds]\n\n# Print the classification report\nprint(classification_report(y_test, final_preds))\n\n# Print Precision, Recall and F1 Score\nprint(\"Precision: %f\" % precision_score(y_test, final_preds))\nprint(\"Recall: %f\" % recall_score(y_test, final_preds))\nprint(\"F1 Score: %f\" % f1_score(y_test, final_preds))","metadata":{"_uuid":"22d990d8-542f-432c-b060-ea71ac47e8c9","_cell_guid":"1bca2d40-6494-4388-a7fc-7e90c82c7b1c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:44:11.85051Z","iopub.execute_input":"2024-04-02T07:44:11.851126Z","iopub.status.idle":"2024-04-02T07:44:12.193602Z","shell.execute_reply.started":"2024-04-02T07:44:11.851067Z","shell.execute_reply":"2024-04-02T07:44:12.192248Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_cb = cb.predict(X_val)\npreds_lr = lr.predict(X_val)\npreds_gb = gb.predict(X_val)\npreds_rf = rf.predict(X_val)\npreds_lgbm = lgbm.predict(X_val)\npreds_et = et.predict(X_val)\npreds_xgb = xgb.predict(X_val)\n# Stack the predictions together\npreds = np.vstack([preds_cb, preds_lr, preds_gb, preds_rf, preds_lgbm,preds_et,preds_xgb])\n\n# Calculate the weighted average of predictions\nfinal_preds = np.average(preds, axis=0, weights=res['x'])\n\n# Convert probabilities to class labels\nfinal_preds = [1 if prob > 0.5 else 0 for prob in final_preds]\n\n# Print the classification report\nprint(classification_report(y_val, final_preds))\n\n# Print Precision, Recall and F1 Score\nprint(\"Precision: %f\" % precision_score(y_val, final_preds))\nprint(\"Recall: %f\" % recall_score(y_val, final_preds))\nprint(\"F1 Score: %f\" % f1_score(y_val, final_preds))","metadata":{"_uuid":"36df3aca-ae18-4c5b-aa39-0fe0883ffee2","_cell_guid":"69caf12c-ce1c-4453-8d49-0d2fd3bd34b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:44:17.273172Z","iopub.execute_input":"2024-04-02T07:44:17.274224Z","iopub.status.idle":"2024-04-02T07:44:17.445095Z","shell.execute_reply.started":"2024-04-02T07:44:17.274166Z","shell.execute_reply":"2024-04-02T07:44:17.443658Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.optimize import minimize\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\npreds_cb = cb.predict(X_val)\npreds_lr = lr.predict(X_val)\npreds_gb = gb.predict(X_val)\npreds_rf = rf.predict(X_val)\npreds_lgbm = lgbm.predict(X_val)\npreds_et = et.predict(X_val)\npreds_xgb = xgb.predict(X_val)\n# Stack predictions\npreds = np.vstack([preds_cb, preds_lr, preds_gb, preds_rf, preds_lgbm,preds_et,preds_xgb])\n\ndef loss_func(weights):\n    \n    final_prediction = np.average(preds, axis=0, weights=weights)\n    # Convert probabilities to class labels\n    final_prediction = [1 if prob > 0.5 else 0 for prob in final_prediction]\n    return 1 - f1_score(y_val, final_prediction, average='weighted')\n\n# Our weights are bound between 0 and 1\nbounds = [(0, 1)]*7\n\n# We want our weights to sum to 1\ncons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n\n# Number of random starting points\nnum_starts = 10\n\nbest_score = np.inf\nbest_weights = None\n\n# Perform optimization with several randomly chosen starting points\nfor _ in range(num_starts):\n    # Randomly choose starting weights\n    values = np.random.rand(7)\n    starting_values = values / np.sum(values)\n\n    res = minimize(loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n\n    if res.fun < best_score:\n        best_score = res.fun\n        best_weights = res.x\n\n# Calculate the weighted average of predictions\nfinal_preds = np.average(preds, axis=0, weights=best_weights)\n\n# Convert probabilities to class labels\nfinal_preds = [1 if prob > 0.5 else 0 for prob in final_preds]\n\nprint('Best Ensemble Weights: {weights}'.format(weights=best_weights))\n\nprint(classification_report(y_test, final_preds))\n\n# Print Precision, Recall and F1 Score\nprint(\"Precision: %f\" % precision_score(y_test, final_preds))\nprint(\"Recall: %f\" % recall_score(y_test, final_preds))\nprint(\"F1 Score: %f\" % f1_score(y_test, final_preds))","metadata":{"_uuid":"2a819d78-1a02-4c26-8df9-16bd408c9e3f","_cell_guid":"9fd74360-781c-4bbf-a4b2-5ec71baab624","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:44:22.302939Z","iopub.execute_input":"2024-04-02T07:44:22.30345Z","iopub.status.idle":"2024-04-02T07:44:22.699166Z","shell.execute_reply.started":"2024-04-02T07:44:22.303394Z","shell.execute_reply":"2024-04-02T07:44:22.697774Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_cb = cb.predict(X_val)\npreds_lr = lr.predict(X_val)\npreds_gb = gb.predict(X_val)\npreds_rf = rf.predict(X_val)\npreds_lgbm = lgbm.predict(X_val)\npreds_et = et.predict(X_val)\npreds_xgb = xgb.predict(X_val)\n# Stack the predictions together\npreds = np.vstack([preds_cb, preds_lr, preds_gb, preds_rf, preds_lgbm,preds_et,preds_xgb])\n\n# Calculate the weighted average of predictions\nfinal_preds = np.average(preds, axis=0, weights=best_weights)\n\n# Convert probabilities to class labels\nfinal_preds = [1 if prob > 0.5 else 0 for prob in final_preds]\n\n# Print the classification report\nprint(classification_report(y_val, final_preds))\n\n# Print Precision, Recall and F1 Score\nprint(\"Precision: %f\" % precision_score(y_val, final_preds))\nprint(\"Recall: %f\" % recall_score(y_val, final_preds))\nprint(\"F1 Score: %f\" % f1_score(y_val, final_preds))","metadata":{"_uuid":"ac12c6b2-8c3d-48a3-84fa-afb62066e48e","_cell_guid":"306bfbb4-c2cc-437f-89dd-ae68c21ee647","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:44:27.838446Z","iopub.execute_input":"2024-04-02T07:44:27.838985Z","iopub.status.idle":"2024-04-02T07:44:28.013954Z","shell.execute_reply.started":"2024-04-02T07:44:27.838941Z","shell.execute_reply":"2024-04-02T07:44:28.012538Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\nmodels = [cb, lr, gb,rf,lgbm,et,xgb]\nmodel_names = ['cb', 'lr', 'gb', 'rf', 'lgbm','et','xgb']\n\n# Get the predictions from each model on the validation set\npreds = [model.predict(X_val) for model in models]\n\n# Calculate Disagreement Measure and Correlation of Errors\nfor i in range(len(models)):\n    for j in range(i+1, len(models)):\n        # Disagreement Measure\n        disagree = np.mean(preds[i] != preds[j])\n        print(f'Disagreement Measure between {model_names[i]} and {model_names[j]}: {disagree}')\n        \n        # Correlation of Errors\n        errors_i = preds[i] != y_val\n        errors_j = preds[j] != y_val\n        correlation = np.corrcoef(errors_i, errors_j)[0, 1]\n        print(f'Correlation of Errors between {model_names[i]} and {model_names[j]}: {correlation}')","metadata":{"_uuid":"7539c6be-a9a9-4cd9-a389-f21d9517a729","_cell_guid":"f6075caf-6e25-4d3b-b2aa-e4e442bd0513","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T07:44:36.650929Z","iopub.execute_input":"2024-04-02T07:44:36.65147Z","iopub.status.idle":"2024-04-02T07:44:36.823856Z","shell.execute_reply.started":"2024-04-02T07:44:36.651426Z","shell.execute_reply":"2024-04-02T07:44:36.822372Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WEBAPP\n### Store this in html file","metadata":{"_uuid":"60f5396b-c1fc-4065-9111-4786a62dd114","_cell_guid":"82af3c98-7043-474e-964a-e3eeca06fd97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"<!DOCTYPE html>\n<html>\n<head>\n    <title>Heart Disease Prediction</title>\n    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n</head>\n<body>\n    <h1>Heart Disease Prediction</h1>\n    <form id=\"predict-form\">\n        <label for=\"age\">Age:</label><br>\n        <input type=\"number\" id=\"age\" name=\"age\"><br>\n        <label for=\"sex\">Sex:</label><br>\n        <select id=\"sex\" name=\"sex\">\n            <option value=\"Male\">Male</option>\n            <option value=\"Female\">Female</option>\n        </select><br>\n        <label for=\"chestPainType\">Chest Pain Type:</label><br>\n        <select id=\"chestPainType\" name=\"chestPainType\">\n            <option value=\"Typical angina\">Typical angina</option>\n            <option value=\"Atypical angina\">Atypical angina</option>\n            <option value=\"Non-anginal pain\">Non-anginal pain</option>\n            <option value=\"Asymptomatic\">Asymptomatic</option>\n        </select><br>\n        <label for=\"restingBP\">Resting Blood Pressure:</label><br>\n        <input type=\"number\" id=\"restingBP\" name=\"restingBP\"><br>\n        <label for=\"cholesterol\">Cholesterol:</label><br>\n        <input type=\"number\" id=\"cholesterol\" name=\"cholesterol\"><br>\n        <label for=\"fastingBS\">Fasting Blood Sugar:</label><br>\n        <input type=\"number\" id=\"fastingBS\" name=\"fastingBS\"><br>\n        <label for=\"maxHR\">Maximum Heart Rate:</label><br>\n        <input type=\"number\" id=\"maxHR\" name=\"maxHR\"><br>\n        <label for=\"exerciseAngina\">Exercise Induced Angina:</label><br>\n        <select id=\"exerciseAngina\" name=\"exerciseAngina\">\n            <option value=\"Yes\">Yes</option>\n            <option value=\"No\">No</option>\n        </select><br>\n        <input type=\"submit\" value=\"Predict\">\n    </form>\n    <p id=\"prediction\"></p>\n\n    <script>\n        $(\"#predict-form\").submit(function(event) {\n            event.preventDefault();\n            var data = {\n                'Age': parseInt($(\"#age\").val()),\n                'Sex': $(\"#sex\").val(),\n                'ChestPainType': $(\"#chestPainType\").val(),\n                'RestingBP': parseInt($(\"#restingBP\").val()),\n                'Cholesterol': parseInt($(\"#cholesterol\").val()),\n                'FastingBS': parseInt($(\"#fastingBS\").val()),\n                'MaxHR': parseInt($(\"#maxHR\").val()),\n                'ExerciseAngina': $(\"#exerciseAngina\").val()\n            };\n            $.ajax({\n                url: '/predict',\n                method: 'POST',\n                contentType: 'application/json',\n                data: JSON.stringify(data),\n                success: function(response) {\n                    $(\"#prediction\").text('Prediction: ' + response.prediction);\n                },\n                error: function(response) {\n                    $(\"#prediction\").text('Error: ' + response.responseJSON.error);\n                }\n            });\n        });\n        \n    </script>\n</body>\n</html>","metadata":{"_uuid":"fd130cb0-f739-40e5-a1c3-0debe3e1d668","_cell_guid":"448e28e2-78fc-4df0-9e06-b148fe6914e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nimport pickle\nimport numpy as np\n\n# Load the models\ncb1 = pickle.load(open('cbheart1.pkl', 'rb'))\ncb2 = pickle.load(open('cbheart2.pkl', 'rb'))\nlr = pickle.load(open('LRHD_0.8F1_0.75Acc.pkl', 'rb'))\ngb = pickle.load(open('GradBoostHeartDisease_0.82_0.82.pkl', 'rb'))\n\n# Define the ensemble weights\nweights = np.array([0.25014892, 0.24990093, 0.24985107, 0.25009908])\n\napp = Flask(__name__)\ndef validate_input(data):\n    # Convert input data to the format expected by the models\n    data['Sex'] = 1 if data['Sex'].lower() == 'male' else 0\n    chest_pain_types = ['typical angina', 'atypical angina', 'non-anginal pain', 'asymptomatic']\n    for i, chest_pain_type in enumerate(chest_pain_types):\n        data['ChestPainType_' + str(i)] = data['ChestPainType'].lower() == chest_pain_type\n    del data['ChestPainType']\n    data['FastingBS'] = 1 if data['FastingBS'] > 120 else 0\n    data['ExerciseAngina'] = 1 if data['ExerciseAngina'].lower() == 'yes' else 0\n\n    # Convert the data to a flat list of features\n    features = [data['Age'], data['Sex'], data['RestingBP'], data['Cholesterol'], data['MaxHR'], data['ExerciseAngina']] + [data['ChestPainType_' + str(i)] for i in range(4)]\n\n    # Define the expected data types for each feature\n    expected_types = [int, int, int, int, int, int, bool, bool, bool, bool]\n    constraints = [(20,80), (0,1), (0,200), (0,603), (60,202), (0,1), (False,True), (False,True), (False,True), (False,True)]\n    \n    # Check if the number of features is correct\n    if len(features) != len(expected_types):\n        return False, \"Incorrect number of features. Expected {} but got {}.\".format(len(expected_types), len(features)), None\n    \n    # Check the data type and constraints of each feature\n    for i in range(len(features)):\n        if type(features[i]) != expected_types[i]:\n            return False, \"Incorrect data type for feature {}. Expected {} but got {}.\".format(i, expected_types[i].__name__, type(features[i]).__name__), None\n        if features[i] < constraints[i][0] or features[i] > constraints[i][1]:\n            return False, \"Feature {} out of bounds. Expected between {} and {} but got {}.\".format(i, constraints[i][0], constraints[i][1], features[i]), None\n    # If all checks pass, return True and the features\n    return True, \"Input is valid.\", features\n\n        \n@app.route('/',methods=[\"GET\"])\ndef home():\n    return app.send_static_file('index.html')\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # Get the data from the POST request\n    data = request.get_json(force=True)\n    is_valid, message, features = validate_input(data)\n    if not is_valid:\n        print(message)\n        return jsonify({'error': message}),400\n    # Make prediction using the models\n    prediction1 = cb1.predict_proba([np.array(features)])\n    prediction2 = cb2.predict_proba([np.array(features)])\n    prediction3 = lr.predict_proba([np.array(features)])\n    prediction4 = gb.predict_proba([np.array(features)])\n\n    # Compute the ensemble prediction\n    ensemble_prediction = np.argmax(np.average(np.array([prediction1, prediction2, prediction3, prediction4]), axis=0, weights=weights))\n\n    # Send back to the client\n    output = {'prediction': int(ensemble_prediction)}\n    return jsonify(output)\n\nif __name__ == '__main__':\n    app.run(port=5000, debug=True)","metadata":{"_uuid":"32cd7fc1-bda2-43c8-89fe-de74129e6e09","_cell_guid":"5b34c600-e3a7-4318-ba2e-e36f9e6e3ecc","trusted":true,"collapsed":false,"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null}]}